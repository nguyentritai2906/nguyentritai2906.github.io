[{"content":"There are not much infomation on the internet on how to visualize your embeddings with Tensorboard. I\u0026rsquo;ve had a hard time trying to make it work properly. Although there\u0026rsquo;re some tutorial on the internet but most of them are for Tensorflow 1 and I didn\u0026rsquo;t find the Tensorboard guide documentation page much help. So here\u0026rsquo;s how I make it works and hopefully it\u0026rsquo;ll help you too. But before we continue, I\u0026rsquo;ll assume you already have an image dataset with labels and a model ready.\nOkay let\u0026rsquo;s dive in. Here\u0026rsquo;s what we\u0026rsquo;ll need to do:\n Use our model to predict the training set and take the output of embedding layer. Create a sprite image for projecting image on Tensorboard. Save the labels (and others if avalable) to a metadata.tsv file Save a tensorflow checkpoint with embedding vectors as tf.Variable  All these files must be saved in the same folder since later we\u0026rsquo;ll call Tensorboard and specify this directory as log_dir for it to find the necessary files.\nFeature extract model First create your model and load the appropriate weight. Mine is just a pretrain ResNet50 with an additional dense layer with 64 neurons as my embedding layer.\nmodel = create_model(cfg) log_dir = \u0026#39;embedding_log\u0026#39; ckpt_path = tf.train.latest_checkpoint(\u0026#39;./checkpoints/ckpt0\u0026#39;) if ckpt_path is not None: print(\u0026#34;Load ckpt from {}\u0026#34;.format(ckpt_path)) model.load_weights(ckpt_path).expect_partial() else: raise ValueError(\u0026#34;No ckpt found\u0026#34;) Next we\u0026rsquo;ll save the model embedding vectors for future runs. Here I\u0026rsquo;ll loop through the train dataset, predict on a batch of images and then save the embedding vectors, labels and images to their appropriate list. If you have the labels and images ready, you can just call model.predict(train_dataset) directly. It\u0026rsquo;ll process the whole dataset and output an embedding matrix. This way, you don\u0026rsquo;t have to loop through the dataset like I do here.\nimage_list = [] label_list = [] embedding_list = [] for batch in tqdm(train_dataset, total=steps_per_epoch + 1): images, labels = batch[0] embeddings = model.predict((images, labels)) embedding_list.append(embeddings) label_list.append(np.argmax(labels, axis=1)) image_list.append(images.numpy() * 255) embeddings = np.concatenate(embedding_list, axis=0) labels = np.concatenate(label_list, axis=0) images = np.concatenate(image_list, axis=0) # Should save these for future runs # Use an if-else block to check if the files exist # So you don\u0026#39;t have to loop through the dataset every run. np.save(os.path.join(log_dir, \u0026#39;embeddings.npy\u0026#39;), embeddings) print(\u0026#34;Saved embeddings to {}\u0026#34;.format(os.path.join(log_dir, \u0026#39;embeddings.npy\u0026#39;))) np.save(os.path.join(log_dir, \u0026#39;labels.npy\u0026#39;), labels) print(\u0026#34;Saved labels to {}\u0026#34;.format(os.path.join(log_dir, \u0026#39;labels.npy\u0026#39;))) np.save(os.path.join(log_dir, \u0026#39;images.npy\u0026#39;), images) print(\u0026#34;Saved images to {}\u0026#34;.format(os.path.join(log_dir, \u0026#39;images.npy\u0026#39;))) print(\u0026#34;Embeddings shape: {}\u0026#34;.format(embeddings.shape)) print(\u0026#34;Labels shape: {}\u0026#34;.format(labels.shape)) print(\u0026#34;Images shape: {}\u0026#34;.format(images.shape)) \u0026gt;\u0026gt;\u0026gt; Embeddings shape: (100, 64) \u0026gt;\u0026gt;\u0026gt; Labels shape: (100, ) \u0026gt;\u0026gt;\u0026gt; Images shape: (100, 256, 256, 3) $ ls -alh embedding_log/ drwxr-xr-x 3 nttai nttai 4.0K Jun 11 15:00 . drwxrwxr-x 18 nttai nttai 4.0K Jun 10 10:23 .. -rw-r--r-- 1 nttai nttai 7.5M Jun 10 10:24 embeddings.npy -rw-r--r-- 1 nttai nttai 45G Jun 10 10:27 images.npy -rw-r--r-- 1 nttai nttai 481K Jun 10 10:27 labels.npy Sprite image Next we\u0026rsquo;ll have to create a sprite image so that when we project its embedding vector onto three dimensional space, Tensorboard will use these thumbnails we create instead of just a bunch of dots. Just make sure that the indexes stay the same.\nKeep in mind that Tensorboard only accept sprite image with maximum dimension of 8192x8192 pixels so you should select only a subset of the training set. Here, since my train_dataset only have 100 images, I\u0026rsquo;ll save each thumbnail with size of 256x256 pixels.\ndef images_to_sprite(images, log_dir, size): one_square_size = int(np.ceil(np.sqrt(len(images)))) master_width = size * one_square_size master_height = size * one_square_size spriteimage = Image.new( mode=\u0026#39;RGBA\u0026#39;, size=(master_width, master_height), color=(0, 0, 0, 0) ) print(\u0026#34;Writing sprite image to {}\u0026#34;.format( os.path.join(log_dir, \u0026#39;sprite.jpg\u0026#39;))) for count, image in tqdm(enumerate(images), total=len(images)): div, mod = divmod(count, one_square_size) h_loc = size * div w_loc = size * mod image = image.squeeze() image = Image.fromarray(image.astype(np.uint8)).resize((size, size)) spriteimage.paste(image, (w_loc, h_loc)) spriteimage.convert(\u0026#39;RGB\u0026#39;).save(f\u0026#39;{log_dir}/sprite.jpg\u0026#39;, transparency=0) thumbnail_size = 256 images_to_sprite(images, log_dir, thumbnail_size)  Fashion sprite image\n  $ ls -alh embedding_log/ drwxr-xr-x 3 nttai nttai 4.0K Jun 11 15:00 . drwxrwxr-x 18 nttai nttai 4.0K Jun 10 10:23 .. -rw-r--r-- 1 nttai nttai 7.5M Jun 10 10:24 embeddings.npy -rw-r--r-- 1 nttai nttai 45G Jun 10 10:27 images.npy -rw-r--r-- 1 nttai nttai 481K Jun 10 10:27 labels.npy -rw-r--r-- 1 nttai nttai 144K Jun 11 15:00 sprite.jpg Metadata to tsv file If you have class name or image id and you want to display it rather than its class label you can write those as another column. Note that when there\u0026rsquo;re more than one column, you\u0026rsquo;re required to write the column labels as the first row, separated by tabs like so.\nlabel name image_path 1 0 cat images/cat/1.jpg 2 1 dog images/dog/1.jpg 3 1 dog images/dog/2.jpg ... ... ... ... Since I only have image labels, I\u0026rsquo;ll use the writerows() method of python csv package.\nwith open(f\u0026#39;{log_dir}/metadata.tsv\u0026#39;, \u0026#39;w\u0026#39;) as fw: csv_writer = csv.writer(fw, delimiter=\u0026#34;\\t\u0026#34;) csv_writer.writerows(np.expand_dims(labels, axis=1)) print(\u0026#34;Labels saved to {}\u0026#34;.format(os.path.join(log_dir, \u0026#39;metadata.tsv\u0026#39;))) Values inside metadata.tsv should look something like this.\n2304 1\t865 2\t3224 3\t1440 4\t3122 5\t4285 6\t334 7\t5282 8\t1744 9\t1026 10\t6774 ... ... $ ls -alh embedding_log/ drwxr-xr-x 3 nttai nttai 4.0K Jun 11 15:00 . drwxrwxr-x 18 nttai nttai 4.0K Jun 10 10:23 .. -rw-r--r-- 1 nttai nttai 7.5M Jun 10 10:24 embeddings.npy -rw-r--r-- 1 nttai nttai 45G Jun 10 10:27 images.npy -rw-r--r-- 1 nttai nttai 481K Jun 10 10:27 labels.npy -rw-r--r-- 1 nttai nttai 585 Jun 11 15:00 metadata.tsv -rw-r--r-- 1 nttai nttai 144K Jun 11 15:00 sprite.jpg Embedding checkpoint Now create a tf.Variable with the embeddings we extracted above and save it to the same log_dir directory.\nembeddings = tf.Variable(embeddings, name=\u0026#39;embeddings\u0026#39;) checkpoint = tf.train.Checkpoint(embedding=embeddings) checkpoint.save(os.path.join(log_dir, \u0026#34;embedding.ckpt\u0026#34;)) $ ls -alh embedding_log/ drwxr-xr-x 3 nttai nttai 4.0K Jun 11 15:00 . drwxrwxr-x 18 nttai nttai 4.0K Jun 10 10:23 .. -rw-r--r-- 1 nttai nttai 89 Jun 11 15:00 checkpoint -rw-r--r-- 1 nttai nttai 13K Jun 11 15:00 embedding.ckpt-1.data-00000-of-00001 -rw-r--r-- 1 nttai nttai 264 Jun 11 15:00 embedding.ckpt-1.index -rw-r--r-- 1 nttai nttai 7.5M Jun 10 10:24 embeddings.npy -rw-r--r-- 1 nttai nttai 45G Jun 10 10:27 images.npy -rw-r--r-- 1 nttai nttai 481K Jun 10 10:27 labels.npy -rw-r--r-- 1 nttai nttai 585 Jun 11 15:00 metadata.tsv -rw-r--r-- 1 nttai nttai 144K Jun 11 15:00 sprite.jpg Projector config The last thing we\u0026rsquo;ll need is the Tensorboard projector config file which specify:\n  The tensor_name. Don\u0026rsquo;t use embeddings.name, hardcode this line like below. Otherwise it\u0026rsquo;ll only project a dot for each image.\n  Name of our metadata file.\n  Name of our sprite image.\n  And specify the size of each thumbnail. Tensorboard will use this to split up each of our thumbnails using the sprite image.\n  config = projector.ProjectorConfig() embedding = config.embeddings.add() # NOTE: Hardcode this line embedding.tensor_name = \u0026#39;embedding/.ATTRIBUTES/VARIABLE_VALUE\u0026#39; embedding.metadata_path = \u0026#39;metadata.tsv\u0026#39; embedding.sprite.image_path = \u0026#39;sprite.jpg\u0026#39; embedding.sprite.single_image_dim.extend([thumbnail_size, thumbnail_size]) projector.visualize_embeddings(log_dir, config) projector_config.pbtxt file should contain these fields.\nembeddings { tensor_name: \u0026#34;embedding/.ATTRIBUTES/VARIABLE_VALUE\u0026#34; metadata_path: \u0026#34;metadata.tsv\u0026#34; sprite { image_path: \u0026#34;sprite.jpg\u0026#34; single_image_dim: 256 single_image_dim: 256 } } $ ls -alh embedding_log/ drwxr-xr-x 3 nttai nttai 4.0K Jun 11 15:00 . drwxrwxr-x 18 nttai nttai 4.0K Jun 10 10:23 .. -rw-r--r-- 1 nttai nttai 89 Jun 11 15:00 checkpoint -rw-r--r-- 1 nttai nttai 13K Jun 11 15:00 embedding.ckpt-1.data-00000-of-00001 -rw-r--r-- 1 nttai nttai 264 Jun 11 15:00 embedding.ckpt-1.index -rw-r--r-- 1 nttai nttai 7.5M Jun 10 10:24 embeddings.npy -rw-r--r-- 1 nttai nttai 45G Jun 10 10:27 images.npy -rw-r--r-- 1 nttai nttai 481K Jun 10 10:27 labels.npy -rw-r--r-- 1 nttai nttai 585 Jun 11 15:00 metadata.tsv -rw-r--r-- 1 nttai nttai 197 Jun 11 15:00 projector_config.pbtxt -rw-r--r-- 1 nttai nttai 144K Jun 11 15:00 sprite.jpg The result Now that we have all required files, let\u0026rsquo;s run Tensorboard in this directory and checkout the result.\n$ tensorboard --logdir embedding_log/ ... TensorBoard 2.9.0 at http://localhost:6006/ (Press CTRL+C to quit) Open your browser and go to http://localhost:6006 and it should look some thing like this.\n Tensorboard Projector with 100 images\n    ","permalink":"https://nguyentritai.tk/posts/2022/tensorboard-projector-image-visualizer/","summary":"There are not much infomation on the internet on how to visualize your embeddings with Tensorboard. I\u0026rsquo;ve had a hard time trying to make it work properly. Although there\u0026rsquo;re some tutorial on the internet but most of them are for Tensorflow 1 and I didn\u0026rsquo;t find the Tensorboard guide documentation page much help. So here\u0026rsquo;s how I make it works and hopefully it\u0026rsquo;ll help you too. But before we continue, I\u0026rsquo;ll assume you already have an image dataset with labels and a model ready.","title":"Tensorboard Projector Image Visualizer with Tensorflow 2"},{"content":"Convolution is one of the fundamental building block of Deep Neural Network (DNN) but sometimes, it can be extremely expensive to compute since it brings a lots of parameters and we are running risk of overfiting. This is where depthwise separable convolution can be used to reduce the total number of parameters, as a result, speed up convolution. This is a form of factorized convolutions which factorize a standard convolution into a depthwise convolution and a 1 x 1 convolution called a pointwise convolution.\nWhat is Depthwise Separable Convolution? Depthwise separable convolution first use the depthwise convolution to applies a single filter to each input channel. Then the pointwise convolution is used to applies a 1 x 1 convolution to combine the outputs of the depthwise convolution. This differ from the standard convolution since it both filters and combines inputs into a new set of outputs in one step, thus more expensive in computation.\nThis depthwise separable convolution splits this into two layers, a separate layer for filtering and a separate layer for combining. This factorization has the effect of drastically reducing computation and model size.\nStandard convolution A standard convolution layer takes as input a $D_F \\times D_F \\times M$ feature map $F$ and produces a $D_F \\times D_F \\times N$ feature map $G$ where $D_F$ is the spatial width and height of a square input feature map (assume that the output feature map has the same spatial dimensions as the input and both feature maps are square), $M$ is the number of input channels (input depth), and $N$ is the number of output channel (output depth).\nThe standard convolutional layer is parameterized by convolution kernel $K$ of size $D_K \\times D_K \\times M \\times N$ where $D_K$ is the spatial dimension of the kernel assumed to be square and $M$ is the number of input channels and $N$ is the number of output channels.\nThe output feature map for standard convolution assuming stride of one and padding is computed as: $$ G_{k,l,n} = \\sum_{i,j,n} K_{i,j,m,n} \\cdot F_{k+i-1,l+j-1,m} $$\nstandard convolutions have the computational cost of: $$ D_K \\cdot D_K \\cdot D_F \\cdot D_F \\cdot M \\cdot N $$\nThat is, for one convolution operation, the number of multiplications is the number of elements in that kernel so that would be $D_K \\times D_K \\times M$ multiplications. But we slide this kernel over the input, we perform $D_K$ convolutions along the width and $D_K$ convolutions along the height and hence $D_K \\times D_K$ convolutions over all. So the number of multiplications in the convolution of one kernel over the entire input $F$ is: $$ D_K \\cdot D_K \\cdot D_F \\cdot D_F \\cdot M $$ Now this is just one kernel but if we have $N$ such kernels which makes the absolute total number of multiplications become: $$ D_K \\cdot D_K \\cdot D_F \\cdot D_F \\cdot M \\cdot N $$\nDepthwise convolution The standard convolution operation has the effect of filtering features based on the convolutional kernels and combining features in order to produce a new representation. The filtering and combination steps can be split into two steps via the use of factorized convolutions called depthwise separable convolutions for substantial reduction in computational cost.\nDepthwise separable convolution are made up of two layers: depthwise convolution and pointwise convolutions. The depthwise convolution to apply a single filter per each input channel (input depth). Pointwise convolution, a simple 1 x 1 convolution, is then used to create a linear combination of the output of the depthwise layer.\nFirst it uses depthwise convolutions to break the interaction between the number of output channels and the size of the kernel. Depthwise convolution with one filter per input channel (input depth) can be written as: $$ \\hat G_{k,l,m} = \\sum_{i,j} \\hat K_{i,j,m} \\cdot F_{k+i-1,l+j-1,m} $$\nWhere $\\hat{K}$ is the depthwise convolutional kernel of size $D_K \\times D_K \\times M$ where the $m^{th}$ filter in $\\hat{K}$ is applied to the $m^{th}$ channel in $F$ to produce the $m^{th}$ channel of the filtered output feature map $\\hat{G}$.\nDepthwise convolution has a computational cost of: $$ D_K \\cdot D_K \\cdot D_F \\cdot D_F \\cdot M $$\nFor depthwise convolution we use filters or kernels of shape $D_K \\times D_K \\times 1$ here it has a depth of one because this convolution is only applied to a channel, unlike standard convolution which is applied throughout the entire depth. Since we apply one kernel to a single input channel, we require $M$ such $D_K \\times D_K \\times 1$ over the entire input volume $F$ for each of these $M$ convolutions we end up with an output $D_F \\times D_F \\times 1$ in shape. Now stacking these outputs together we have an output volume of $G$ which is of shape $D_F \\times D_F \\times M$\nDepthwise convolution is extremely efficient relative to standard convolution. However it only filters input channels, it does not combine them to create new features. So an additional layer that computes a linear combination of the output of depthwise convolution via 1 x 1 convolution is needed in order to generate these new features.\nPointwise convolution Pointwise convolution involves performing the linear combination of each of these layers. Here the input is the volume of shape $D_F \\times D_F \\times M$, the filter has a shape of $1 \\times 1 \\times M$. This is basically a 1 x 1 convolution operation over all $M$ layers. The output this have the same input width and height as the input $D_F \\times D_F$ for each filter. Assuming that we want to use some $N$ such filters, the output volume becomes $D_F \\times D_F \\times N$.\nNow let\u0026rsquo;s take a look at the complexity of this convolution. We can split this into two parts as we have two phases. First we compute the number of multiplications in depthwise convolution. So here the kernels have a shape of $D_K \\times D_K \\times 1$ so the number of multiplications on one convolution operation is $D_K \\times D_K$. When applied over the entire input channel this convolution is performed $D_F \\times D_F$ number of times. So the total multiplications for the kernel over the input channels becomes $D_F \\times D_F \\times D_K \\times D_K$. Now such multiplications are applies over all $M$ input channels. For each input channel we have a difference kernel and hence the total number of multiplications in the first phase that is depthwise convolution is $D_F \\times D_F \\times D_K \\times D_K \\times M$. Next we compute the number of multiplications in the second phase that is pointwise convolution. Here the kernels have a shape of $1 \\times 1 \\times M$ where M is the depth of the input volume. Hence the number of multiplications for one instance of convolution is $M$. This is applied to the entire output of the first phase which has a width and height of $D_F$ so the total number of multiplications for this kernel is $D_F \\times D_F \\times M$. So for some $N$ kernels we will have $N \\times D_F \\times D_F \\times M$ such multiplications and thus the total number of multiplications in the depthwise convolution stage plus the number of multiplications in the pointwise convolution stage.\nHow exactly is this better? Depthwise separable convolutions cost: $$ D_F \\cdot D_F \\cdot D_K \\cdot D_K \\cdot M + M \\cdot N \\cdot D_F \\cdot D_F $$ By expressing convolution as a two step process of filtering and combining we get a reduction in computation of: $$ \\frac{D_F \\cdot D_F \\cdot D_K \\cdot D_K \\cdot M + M \\cdot N \\cdot D_F \\cdot D_F}{D_K \\cdot D_K \\cdot D_F \\cdot D_F \\cdot M \\cdot N} \\newline = \\frac{D_K \\cdot D_K + M}{D_K \\cdot D_K \\cdot N} \\newline = \\frac{1}{N} + \\frac{1}{D^2_K} $$\nTo put this into perspective of how effective depthwise separable convolution is let us take an example. Consider the output features volume $N$ of 1024 and a kernel size 3, that is $D_K$ is equal to 3. Plugging these values into the relation we get 0.112. In other words depthwise separable convolution uses 8 to 9 times less computation than standard convolution.\n","permalink":"https://nguyentritai.tk/posts/2021/depthwise-separable-convolution/","summary":"Convolution is one of the fundamental building block of Deep Neural Network (DNN) but sometimes, it can be extremely expensive to compute since it brings a lots of parameters and we are running risk of overfiting. This is where depthwise separable convolution can be used to reduce the total number of parameters, as a result, speed up convolution. This is a form of factorized convolutions which factorize a standard convolution into a depthwise convolution and a 1 x 1 convolution called a pointwise convolution.","title":"Depthwise Separable Convolution - How does it works?"},{"content":"Linear Regression is the most basic, well known and well understood supervised learning algorithm. It\u0026rsquo;s probably the first thing that every Machine Learning Engineer and Data Scientist come across as it lays the foundation for other sophisticated learning algorithm. But what is it?\nWhat is Linear Regression? Before knowing what is linear regression, let us get ourselves accustomed to regression. Regression is a method of modeling the relationships between a dependent variable and one or more independent variables.\nIt is used when we want to predict the value of a variable based on the value of other variables. The variable we want to predict is called the dependent variable (also called the target or output). The variables we are using to predict the another variable\u0026rsquo;s value are called the independent variables (also called the features or input). Regression techniques mostly differ based on the number of independent variables and the type of relationship between the dependent and independent variables.\nLinear regression is a linear model, i.e. a model that assumes a linear relationship between the input variables $x$ and the single output variable $y$. More specifically, that $y$ can be calculated from a linear combination of the input variables. When there is a single input variable, the method is referred to as simple linear regression. When there are multiple input variables, literature from statistics often refers to the method as multiple linear regression.\nIn machine learning, linear regression is a supervised learning algorithm where the predicted output is continuous and has a constant slope. It’s used to for forecasting and finding out cause and effect relationship between variables within a continuous range (e.g. sales, price).\n  Linear Regression In Real Life \u0026ndash; by Carolina Bento\n  Linear Regression Model Representation Linear regression model is an attractive model because the representation is so simple. It\u0026rsquo;s just a function of one or more input feature $x$ and an output value $y$. As such, both the input and output value are numeric.\nMore generally, a linear model make prediction by simply computing a weighted sum of the input values plus a constant called the bias term (also called the intercept term), as shown in the equation below:\n$$y = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \u0026hellip; + \\theta_n x_n$$\nIn this equation:\n $y$ is the predicted value. $n$ is the number of features. $x_i$ is the $i^{th}$ feature value. $\\theta_j$ (pronounced theta) is the $j^{th}$ model parameter (including the bias term $\\theta_0$ and the feature weights $\\theta_1$, $\\theta_2$, \u0026hellip;, $\\theta_n$).  This can be written more concisely using a vectorized form as:\n$$y = h_{\\bm\\theta}\\left(\\textbf{x}\\right) = \\bm\\theta^T\\cdot\\textbf{x}$$\nWhere:\n $\\bm\\theta^T$ is the transpose of the model\u0026rsquo;s parameter vector, containing the bias term $\\theta_0$ and the feature weights $\\theta_1$ to $\\theta_n$. $\\textbf{x}$ is the instance\u0026rsquo;s feature vector, containing $x_0$ to $x_n$, with $x_0$ always equal to 1. $\\bm\\theta^T \\cdot \\textbf{x} $ is the dot product of the vectors $\\bm\\theta^T$ and $\\textbf{x}$. $h_{\\bm\\theta}$ is the hypothesis function, using the model parameters $\\theta$.  Our job is to set its parameters (i.e. find $\\bm\\theta$) so that the model best fits the training set. For this purpose we need some metric to measure how well (or poorly) the model fits the training data.\nThe Cost Function Mean Squared Error The most common performance measure of a regression model is the Root Mean Squared Error (RMSE). But in some contexts you may prefer to use another function, e.g. Mean Absolute Error (MAE) if there are many outliers. Here we\u0026rsquo;ll use the Mean Squared Error (MSE) because it is simplier to minimize than the RMSE, and it lead to the same result (because it value that minimizes this function also minimizes its square root).\nThe MSE of a Linear Regression hypothesis $h_{\\bm\\theta}$ on a training set $\\textbf{X}$ (also called the cost function) is calculated using the equation:\n$$ \\text{MSE}\\left(\\textbf{X}, h_{\\bm\\theta}\\right) = \\frac{1}{m} \\displaystyle\\sum_{i=1}^m \\left(\\bm\\theta^T \\textbf x^{(i)} - y^{(i)}\\right)^2 $$\nWhere:\n $m$ is the number of instances in the dataset you are measuring on. $\\bm\\theta^T \\textbf x^{(i)}$ is the predicted value of the model for the $i^{th}$ instance. $y^{(i)}$ is the target value of the $i^{th}$ instance. $\\textbf x^{(i)}$ is a vector of all the feature values $\\textbf{X}$ is a matrix containing all the feature values of all instances in the dataset. There is one instance per row, and the $i^{th}$ row is equal to the transpose of all the features of this instance $\\textbf{x}^{(i)}$ $h_{\\bm\\theta}$ is the model\u0026rsquo;s prediction function parameterized by the vector $\\theta$, also called the hypothesis.  What it does is calculate the difference between the model\u0026rsquo;s prediction and the true target value and square it (hence $\\left(\\bm\\theta^T \\textbf x^{(i)} - y^{(i)}\\right)^2$). Do it for all of the instances in the dataset then take the sum of all the calculated square differences. Divide that sum by the total number of instances in the dataset to get the mean. The larger the error, the larger the MSE and the opposite. We want to find the value of $\\bm\\theta$ to minimize the MSE.\nThe Normal Equation To find the value of $\\bm\\theta$ that minimizes the cost function, there is a closed-form solution - in other words, a mathematical equation that gives the result directly.\nLet us representing the cost function in a vector form, starting with the residual\n$$ \\begin{bmatrix} \\theta^T({x}^0)\\newline \\theta^T({x}^1)\\newline \\vdots \\newline \\theta^T({x}^m)\\newline \\end{bmatrix} - \\begin{bmatrix} y^0\\newline y^1\\newline \\vdots \\newline y^m\\newline \\end{bmatrix} = \\textbf{X} \\bm\\theta - y $$\nBut each residual value is squared. We can not simply square the above expression. As the square of a vector/matix is not equal to the square of each of its values. So to get the squared value, multiply the vector/matrix with its transpose. Therefore the final equation is:\n$$ \\text{MSE}\\left(\\textbf{X}, h_{\\bm\\theta}\\right) = \\frac{1}{m}\\left(\\left(\\textbf{X} \\bm\\theta - y\\right)^T \\left(\\textbf{X} \\bm\\theta - y\\right)\\right) \\newline = \\frac{1}{m}\\left(\\left(\\textbf{X}\\bm\\theta\\right)^T\\textbf{X}\\bm\\theta - y\\left(\\textbf{X}\\bm\\theta\\right)^T - y^T\\textbf{X}\\bm\\theta + y^Ty\\right) \\newline = \\frac{1}{m}\\left(\\bm\\theta^T \\textbf{X}^T \\textbf{X} \\bm\\theta - 2 \\bm\\theta^T \\textbf{X}^T y + y^Ty\\right) $$\nNow to minimize $\\text{MSE}\\left(\\textbf{X}, h_{\\bm\\theta}\\right)$ w.r.t. $\\theta$ we\u0026rsquo;ll set its derivative equal to 0 (i.e., implying that we have a local optimum of the error, it would be the best fit line for the dataset) and solve for $\\theta$. But how do we differentiate this?\nOk, here are the two useful identities we\u0026rsquo;ll need:\n Derivative of a linear function:  $$ \\frac{\\partial}{\\partial\\vec{x}}\\vec{a} \\cdot \\vec{x} = \\frac{\\partial}{\\partial\\vec{x}}\\vec{a}^T \\vec{x} = \\frac{\\partial}{\\partial\\vec{x}}\\vec{x}^T \\vec{a} = \\vec{a} $$\n(If you think back to calculus, this is just $\\frac{d}{dx}ax = a$)\n Derivative of a quadratic function: if A is symmetric, then:  $$ \\frac{\\partial}{\\partial\\vec{x}} \\vec{x}^T A \\vec{x} = 2 A \\vec{x} $$\n(Again, thinking back to calculus, this is just like $\\frac{d}{dx}ax^2 = 2ax$)\nIn case you\u0026rsquo;re wondering what if A is non-symmetric. The more general rule is:\n$$ \\frac{\\partial}{\\partial\\vec{x}} \\vec{x}^T A \\vec{x} = \\left(A + A^T\\right) \\vec{x} $$\nWhich of course is the same thing as $2A\\vec{x}$ when $A$ is symmetric.\nNow back at the cost function, we can see that it\u0026rsquo;s derivative w.r.t. $\\bm\\theta$ noted as $\\nabla_\\theta MSE(\\bm\\theta)$ (also called the gradient vector) is just:\n$$ \\nabla_\\theta MSE(\\bm\\theta) = \\frac{2}{m} \\left(\\textbf{X}^T\\textbf{X}\\bm\\theta - \\textbf{X}^Ty\\right) $$\nLet\u0026rsquo;s set it equal to 0 (noted as $\\stackrel{!}{=}$, think of it as \u0026ldquo;shall be (made) equal to\u0026rdquo;) and solve for $\\bm\\theta$\n$$ \\nabla_\\theta MSE(\\bm\\theta) = \\frac{2}{m} \\left(\\textbf{X}^T\\textbf{X}\\bm\\theta - \\textbf{X}^Ty\\right) \\stackrel{!}{=} 0 \\newline \\implies \\textbf{X}^T\\textbf{X}\\bm\\theta = \\textbf{X}^Ty \\newline \\implies \\bm\\theta = (\\textbf{X}^T \\textbf{X})^{-1} \\ \\ \\textbf{X}^T \\ \\ y $$\nThis is called the Normal Equation:\n$$ \\widehat{\\bm\\theta} = (\\textbf X^T \\textbf X )^{-1} \\ \\ \\textbf X^T \\ \\ y $$\nIn this equation:\n $\\widehat{\\bm\\theta}$ is the value of $\\bm\\theta$ that minimizes the cost function. $y$ is the vector of target values containing $y^{(1)}$ to $y^{(m)}$  Let\u0026rsquo;s generate some random linear-looking data to this equation.\nimport numpy as np X = 2 * np.random.rand(100, 1) y = 5 + 4 * X + np.random.randn(100, 1) Now let compute $\\widehat{\\bm\\theta}$ using the Normal Equation. We\u0026rsquo;ll use the np.linalg.inv() function from NumPy\u0026rsquo;s linear algebra module to compute the inverse of a matrix. Note that the @ symbol was introduced in Python 3.5 as matrix multiplication.\nX_b = np.c_[np.ones((100,1)), X] theta_bestfit = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y The function we used to generate the data is $y = 5 + 4x_1$ with some Gaussian noise. Let\u0026rsquo;s see what the equation found!\n\u0026gt;\u0026gt;\u0026gt; theta_bestfit array([[5.09647054], [4.01624421]]) Close enough. The noise made it impossible to recover the exact parameters of the original function. Now we can make prediction using $\\widehat{\\bm\\theta}$\nX_test = np.array([[0], [3]]) X_test_b = np.c_[np.ones((2,1)), X_test] y_predict = X_test_b @ theta_bestfit \u0026gt;\u0026gt;\u0026gt; y_predict array([[ 5.09647054], [17.14520317]]) But the Normal Equation may not work if the matrix $\\bm{X}^T\\bm{X}$ is not invertible (i.e. singular also called degenerate). This equation has a single solution if $\\bm{X}^T\\bm{X}$ is invertible (i.e. non-singular). If it\u0026rsquo;s not, you have more solutions.\nTake $\\textbf{X} = \\begin{bmatrix} 1 \u0026amp; 0 \\newline 0 \u0026amp; 1 \\newline 0 \u0026amp; 0 \\end{bmatrix}$ for example, it\u0026rsquo;s transpose will be $\\textbf{X}^T = \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \\newline 0 \u0026amp; 1 \u0026amp; 0 \\end{bmatrix} $. Therefore\n$$ \\textbf{X}^T \\textbf{X} = \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \\newline 0 \u0026amp; 1 \u0026amp; 0 \\newline 0 \u0026amp; 0 \u0026amp; 0 \\end{bmatrix} $$\nWe can see that the determinant of this matrix 0 so it is not invertible. Then how can we solve for $\\bm\\theta$? Use the Moore-Penrose inverse!\nThe Moore-Penrose Pseudoinverse In Linear Algebra, the Moore-Penrose inverse of a matrix is the most well known generalization of the inverse matrix. The term generalized inverse is sometimes used as a synonym for pseudoinverse. A common use of the pseudoinverse is to compute a \u0026ldquo;best fit\u0026rdquo; (least square) solution to a system of linear equations that lacks a solution. Another is to find the minimum (Euclidean) norm solution to a system of linear equations with multiple solutions. That\u0026rsquo;s exactly what we need.\nThe pseudoinverse itself is computed using a standard matrix factorization technique called Singular Value Decomposition (SVD) that can decompose any matrix $\\textbf X$ into the matrix multiplication of three matrices.\n$$ \\textbf{X} = \\textbf{U} \\Sigma \\textbf{V}^T $$\nWhere $\\textbf{U}$ and $\\textbf{V}$ are orthogonal matrices (i.e. its inverse equal to its transpose), $\\Sigma$ is a diagonal matrix (i.e. entries out side the main diagonal are zeros).\nWhat does it actually do? You might ask. I\u0026rsquo;ll save it for another post. It\u0026rsquo;s deserve a post of its own. Now let\u0026rsquo;s suppose $\\textbf X$ is invertible, then its inverse is\n$$ \\textbf X^{-1} = \\textbf V \\Sigma^{-1} \\textbf U^T $$\nThe pseudoinverse of $\\textbf{X}$ is quite similar but instead of $\\Sigma^{-1}$ we use it\u0026rsquo;s pseudoinverse $\\Sigma^+$\n$$ \\textbf{X}^+ = \\textbf{V} \\Sigma^+ \\textbf{U}^T $$\nWhere $\\Sigma^+$ is computed by replace all nonzero values with their inverse and transpose the resulting matrix.\nNow let\u0026rsquo;s take our linear model equation above and rewrite it in its matrix form as:\n$$ \\textbf{X} \\bm\\theta = y $$\nWe know that inverse of a matrix multiply by itself is equal to the identity matrix and any matrix multiply by the identity matrix result in the matrix itself. Hence\n$$ \\textbf{X}^+ \\textbf{X} \\bm\\theta = \\textbf{X}^+ y \\newline \\implies \\bm\\theta = \\textbf{X}^+ y $$\nSee something similar? That\u0026rsquo;s right, it looks like our Normal Equation above. It implies that\n$$ \\textbf{X}^+ = \\textbf{V} \\Sigma^+ \\textbf{U}^T = (\\textbf X^T \\textbf X )^{-1} \\ \\ \\textbf X^T $$\nIndeed, if we multiply the left and right side of the equation to $\\textbf{X}$ we\u0026rsquo;ll get back the identity matrix\n$$ \\textbf{X}^+ \\textbf{X} = (\\textbf X^T \\textbf X)^{-1} \\ \\ \\textbf X^T \\textbf{X} = \\textbf I $$\nOr if we take the right hand side of the equation, plug in the SVD and cancel like crazy we\u0026rsquo;ll get\n$$ (\\textbf X^T \\textbf X)^{-1} \\ \\ \\textbf X^T \\newline = (\\textbf U^T \\Sigma \\textbf V \\ \\textbf U \\Sigma \\textbf V^T)^{-1} \\ \\textbf U^T \\Sigma \\textbf V \\newline = \\textbf V \\Sigma^+ \\textbf U^T \\ \\textbf V^T \\Sigma^+ \\textbf U \\ \\textbf U^T \\Sigma \\textbf V \\newline = \\textbf V \\Sigma^+ \\textbf U^T $$\nThis approach is more efficient than computing the Normal Equation, plus it handles edge cases nicely and the pseudoinverse is always defined.\nPerforming Moore-Penrose Pseudoinverse in python is simple. Just call np.linalg.pinv() instead!\ntheta_pinv = np.linalg.pinv(X_b) @ y \u0026gt;\u0026gt;\u0026gt; theta_pinv array([[5.09647054], [4.01624421]]) Computational Complexity The Normal Equation computes the inverse of $\\textbf{X}^T \\textbf X$, which is an $(n + 1) \\times (n + 1)$ matrix (where $n$ is the number of features). The computational complexity of inverting such an matrix is typically about $O(n^{2.4})$ to $O(n^3)$, depending on the implementation. In other words, if you double the number of features, you multiply the computational time by roughly $2^{2.4} = 5.3$ to $2^3 = 8$.\nThe SVD approach used by Scikit-Learn\u0026rsquo;s LinearRegression class is about $O(n^2)$. If you double the number of features, you multiply the computational time by roughly 4.\nWhat it means is both the Normal Equation and the SVD approach get very slow when the number of features grows large (e.g. 100,000). On the positive side, both are linear with regard to the number of instances in the training set (they are $O(m)$), so they handle large training sets efficiently, provided they can fit in memory.\nAlso, once you have trained your Linear Regression model (using the Normal Equation or any other algorithm), predictions are very fast: the computational complexity is linear with regard to both the number of instances you want to make predictions on and the number of features. In other words, making predictions on twice as many instances (or twice as many features) will take roughly twice as much time.\nNow we\u0026rsquo;ll look at a very different way to train a Linear Regression model, which is better suited for cases where there are a large number of features or too many training instances to fit in memory.\nGradient Descent Gradient Descent is a generic optimization algorithm capable of finding optimal solutions to a wide range of problems. The general idea of Gradient Descent is to tweak parameters iteratively in order to minimize a cost function. It measures the local gradient of the error function with regard to the parameter vector $\\theta$, and it goes in the direction of descending gradient. Once the gradient is zero, you have reached a minimum!\nGenerally you start by defining $\\theta$ with random values (this is called random initialization). Then you improve it gradually, taking one baby step at a time, each step attempting to decrease the cost function (e.g. the MSE), until the algorithm converges to a minimum.\nAn important parameter in Gradient Descent is the size of the step, determined by the learning rate hyperparameter. If the learning rate is too small, then the algorithm will have to go through many iterations to converges, which will take a long time.\nOn the other hand, if the learning rate is too high, you might over shoot the local minimum, possibly even higher error than you were before. This might make the algorithm diverge, with larger and larger values, failing to find a good solution.\nFinally, not all cost functions looks like nice, regular bowls. There may be holes, ridges plateaus, and all sort of irregular terrains, making convergence to the minimum difficult.\nFortunately, the MSE cost function for a Linear Regression model happens to be a convex function, which means that if you pick any two points on the curve, the line segment join them never crosses the curve. This implies that there are no local minima, just one global minimum. It is also a continuous function with a slope that never changes abruptly. These two facts have a great consequence: Gradient Descent is guaranteed to approach arbitrary close the global minimum.\nTo implement Gradient Descent, you need to compute the gradient of the cost function with regard to each model parameter $\\theta_j$. In other words, you need to calculate how much the cost function will change if you change $\\theta_j$ just a little bit. This is called the partial derivative. It is like asking \u0026ldquo;If I take one small step ahead, which way is downhill?\u0026rdquo;. The equation to computes the partial derivative of the cost function with regard to parameter $\\theta_j$ noted $\\frac{\\partial}{\\partial\\theta_j}MSE(\\bm\\theta)$ is\n$$ \\frac{\\partial}{\\partial\\theta_j}MSE(\\bm\\theta)\\ = \\frac{2}{m} \\sum_{i=1}^m (\\bm\\theta^T \\textbf{x}^{(i)} - y^{(i)})x_j^{(i)} $$\nInstead of computing these partial derivatives individually, you can use the equation below (just like we\u0026rsquo;ve worked it out above for the Normal Equation) to compute them all in one go. The gradient vector, noted $\\nabla_\\theta MSE(\\bm\\theta)$, contains all the partial derivatives of the cost function (one for each model parameter).\n$$ \\nabla_\\theta MSE(\\bm\\theta) = \\begin{pmatrix} \\frac{\\partial}{\\partial\\theta_0}MSE(\\bm\\theta) \\newline \\frac{\\partial}{\\partial\\theta_1}MSE(\\bm\\theta) \\newline \\vdots \\newline \\frac{\\partial}{\\partial\\theta_n}MSE(\\bm\\theta) \\newline \\end{pmatrix} = \\frac{2}{m} \\textbf X^T (\\textbf X \\bm\\theta - \\bm y) $$\nNotice that this formula involves calculations over the full training set $\\textbf X$, at each Gradient Descent step! As a result it is terribly slow on very large training sets. However, Gradient Descent scales well with the number of features; training a Linear Regression model when there are hundreds of thousands of features is much fast using Gradient Descent than using the Normal Equation or SVD decomposition.\nOnce you have the gradient vector, which increase the MSE, just go in the opposite direction. That means subtracting $\\nabla_{\\bm\\theta} MSE(\\bm\\theta)$ from $\\bm\\theta$. This is where the learning rate $\\eta$ comes in to plays: multiply the gradient vector by $\\eta$ to determine the size of the downhill step.\n$$ \\bm\\theta^{\\text{(next step)}} = \\bm\\theta - \\eta \\nabla_{\\bm\\theta} MSE(\\bm\\theta) $$\nLet\u0026rsquo;s look at a quick implementation of this algorithm.\neta = 0.1 n_iteration = 1000 m = 100 theta = np.random.randn(2, 1) for iteration in range(n_iteration): gradients = 2/m * X_b.T @ (X_b @ theta - y) theta -= eta * gradients That wasn\u0026rsquo;t too hard! Let\u0026rsquo;s take a look at the resulting $\\bm\\theta$\n\u0026gt;\u0026gt;\u0026gt; theta array([[5.09647054], [4.01624421]]) That\u0026rsquo;s exactly what the Normal Equation found! But what if you have used a different learning rates $\\eta$?\nOn the left the learning rate is too low: the algorithm will eventually reach the solution, but it will take a long time. On the right the learning rate is too high: the algorithm diverges, jumping all over the place and actually getting further and further away from the solution at every step. In the middle, the learning rate looks pretty good: in just a few iterations, it has already converged to the solution.\nConclution Linear Regression is an algorithm that every Data Scientist and Machine Learning engineer must know. It is also a good place to start for people who want to get into the Machine Learning industry as well. It is really a simple but useful algorithm.\nIn this post we have learnt about the concepts of linear regression and gradient descent. We also had a deep dive into the mathematics behind it as well as implemented the model from scratch using only NumPy. I hope you had as much fun reading as I did writing it.\nThank you for reading!\n","permalink":"https://nguyentritai.tk/posts/2020/linear-regression/","summary":"Linear Regression is the most basic, well known and well understood supervised learning algorithm. It\u0026rsquo;s probably the first thing that every Machine Learning Engineer and Data Scientist come across as it lays the foundation for other sophisticated learning algorithm. But what is it?\nWhat is Linear Regression? Before knowing what is linear regression, let us get ourselves accustomed to regression. Regression is a method of modeling the relationships between a dependent variable and one or more independent variables.","title":"Linear Regression and a deep dive into the mathematics behind it."},{"content":" Nguyen Tri Tai   A Junior AI R\u0026D Engineer based in Ho Chi Minh, Viet Nam.  Specialize in Computer Vision include Image Recognition, Segmentation and Generation.  Also a book lover, photographer and guitar player.  ","permalink":"https://nguyentritai.tk/about/","summary":" Nguyen Tri Tai   A Junior AI R\u0026D Engineer based in Ho Chi Minh, Viet Nam.  Specialize in Computer Vision include Image Recognition, Segmentation and Generation.  Also a book lover, photographer and guitar player.  ","title":""}]